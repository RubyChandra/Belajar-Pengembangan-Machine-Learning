# -*- coding: utf-8 -*-
"""Submission_Multiclass_Text_Dicoding

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rzmrHcIYvhpiL9ReSsqKV3jzTPdyDJaP

# Library
"""

import os
import re
import tensorflow as tf
import pandas as pd
import nltk
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""# Dataset
BBC NEWS 5 Class, >2000 Sample
https://www.kaggle.com/datasets/mrutyunjaybiswal/iitjee-neet-aims-students-questions-data
"""

!wget --no-check-certificate \
https://raw.githubusercontent.com/amirziai/cse6242-project/master/resources/datasets/bbc-text.csv -O /tmp/bbc-text.csv

"""# Callback
Memberhentikan pembelajaran saat akurasi validation diatas 90%
"""

class stopTraining(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if (logs.get('val_loss')<=0.28 and logs.get('val_accuracy')>=0.90):
      print("Menghentikan proses pembelajaran, Target akurasi sudah tercapai")
      self.model.stop_training=True

myCallback = stopTraining()

"""# Preprocessing Text
Meliputi


*   Cleaning Unneeded Char (Punct and Number)
*   Filtering Stopwords

*   Stemming and Lemmatization




"""

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

!wget --no-check-certificate \
https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt -O /tmp/more-stopwords.txt

my_file = open("/tmp/more-stopwords.txt")
data = my_file.read()
MORE_STOPWORDS = data.split("\n")

STOPWORDS = set(stopwords.words('english'))
STOPWORDS.update(MORE_STOPWORDS)

ps = nltk.stem.PorterStemmer()
ls = nltk.stem.WordNetLemmatizer()

def cleaning_func(text):
    arr=[]
    text = text.lower()
    text = re.sub(r'[^\w\s]',' ', text)
    text = re.sub(r'\d+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    text = text.split()
    for word in text:
      word = ls.lemmatize(word)
      word = ps.stem(word)
      arr.append(word)
    text = ' '.join(arr)
    return text

df = pd.read_csv('/tmp/bbc-text.csv')
category = pd.get_dummies(df.category)

df = pd.concat([df,category], axis=1)
df.drop(columns='category', inplace=True, axis=1)

for i in df.index:
  df.at[i,'text']=cleaning_func(df.at[i,'text'])

text_set = df['text']
label_set = df[['business','entertainment','politics','sport','tech']].values

X_train, X_test, y_train, y_test = train_test_split(text_set,
                                                    label_set,
                                                    random_state=1,
                                                    test_size=0.2,
                                                    shuffle=True
                                                    )

"""Tokenzing -> Sequences -> Pad_Sequences"""

tokenizer = Tokenizer(num_words=5000, oov_token='UNK',
                      filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~',
                      lower=True)
tokenizer.fit_on_texts(X_train) 
     
sekuens_latih = tokenizer.texts_to_sequences(X_train)
sekuens_test = tokenizer.texts_to_sequences(X_test)
     
padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

"""# Model Preparation"""

model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(5, activation='softmax')
    ])
model.compile(loss='categorical_crossentropy',
              optimizer='nadam',
              metrics=['accuracy']
              )

model.summary()

"""# Fitting Model"""

num_epochs = 100
records = model.fit(padded_latih, y_train, epochs=num_epochs, batch_size=28, 
                    validation_data=(padded_test, y_test), verbose=2,
                    callbacks=[myCallback])

"""# PLOT ACCURACY and LOSS"""

f = plt.figure(figsize=(12,3))
plt.subplot(1, 2, 1)
plt.plot(records.history['accuracy'])
plt.plot(records.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(records.history['loss'])
plt.plot(records.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.show()